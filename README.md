# API Performance Testing Framework (Locust)

A comprehensive performance testing framework built on Locust for load testing APIs with support for JWT authentication, configurable scenarios, and SLA validation.

## Features

üöÄ **Production-Ready:**
- ‚úÖ Weighted task distribution for realistic load patterns
- ‚úÖ Automatic JWT token management and refresh
- ‚úÖ Comprehensive error handling and validation
- ‚úÖ SLA validation for both response time and error rates
- ‚úÖ Beautiful HTML and CSV report generation
- ‚úÖ YAML-based configuration for easy management
- ‚úÖ Python 3.7+ support with modern best practices

## Table of Contents
- [Features](#features)
- [Quick Start](#quick-start)
- [Overview](#overview)
- [Project Structure](#project-structure)
- [Installation](#installation)
  - [Python 3 Setup](#python-3-setup)
- [Configuration](#configuration)
- [Running Tests](#running-tests)
- [Test Scenarios](#test-scenarios)
- [SLA Thresholds](#sla-thresholds)
- [Reports & Metrics](#reports--metrics)
- [Understanding Your Reports](#understanding-your-reports)
- [Authentication](#authentication)
- [Troubleshooting](#troubleshooting)
- [Development](#development)
- [License](#license)

## Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Update configuration
# Edit config/env.yaml with your API endpoint and credentials

# 3. Define test scenarios
# Edit scenarios/users_api.yaml with your API endpoints

# 4. Set SLA thresholds
# Edit thresholds/sla.yaml with your performance requirements

# 5. Run tests
python runner/run.py
```

## Overview

This framework provides a flexible, configuration-driven approach to API performance testing. It supports:
- Multiple API endpoints and HTTP methods
- JWT-based authentication
- Dynamic task allocation with weighted load distribution
- Customizable SLA thresholds
- HTML and CSV report generation
- YAML-based configuration for scenarios and thresholds

## Project Structure

```
api-perf-framework/
‚îú‚îÄ‚îÄ auth/                     # Authentication modules
‚îÇ   ‚îî‚îÄ‚îÄ jwt.py              # JWT token handling
‚îú‚îÄ‚îÄ config/                   # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ env.yaml            # Environment and API configuration
‚îú‚îÄ‚îÄ locustfiles/             # Locust test definitions
‚îÇ   ‚îú‚îÄ‚îÄ base_api_user.py    # Base user class for API testing
‚îÇ   ‚îî‚îÄ‚îÄ dynamic_tasks.py    # Dynamic task generation
‚îú‚îÄ‚îÄ runner/                   # Test execution and validation
‚îÇ   ‚îú‚îÄ‚îÄ run.py              # Main test runner
‚îÇ   ‚îî‚îÄ‚îÄ validate.py         # SLA validation
‚îú‚îÄ‚îÄ scenarios/               # Test scenario definitions
‚îÇ   ‚îî‚îÄ‚îÄ users_api.yaml      # API endpoints and test cases
‚îú‚îÄ‚îÄ thresholds/             # SLA configuration
‚îÇ   ‚îî‚îÄ‚îÄ sla.yaml            # Performance thresholds
‚îî‚îÄ‚îÄ requirements.txt        # Python dependencies
```

## Installation

### Prerequisites
- **Python 3.7+** (3.8+ recommended, 3.10+ ideal)
- pip (Python package manager)

### Verify Python Installation

#### macOS/Linux:
```bash
python3 --version
# Output: Python 3.x.x (3.7 or higher)
```

#### Windows:
```bash
python --version
# Output: Python 3.x.x (3.7 or higher)
```

### Python 3 Setup

**Your framework is 100% Python 3 compatible** and uses modern Python 3 features throughout:
- f-strings for clean string formatting
- Context managers for proper resource handling
- Modern libraries (subprocess, json, pathlib, datetime)
- Comprehensive exception handling
- Type hints in docstrings

#### Recommended Python Versions:
- **Python 3.8**: Excellent stability, widely used
- **Python 3.9+**: Good performance improvements
- **Python 3.10+**: Recommended for best experience
- **Python 3.11+**: Latest, fastest version

### Installation Steps

#### Step 1: Install Dependencies
```bash
# Using pip3 (recommended)
pip3 install -r requirements.txt

# Or with Python module
python3 -m pip install -r requirements.txt
```

#### Step 2: Verify Installation
```bash
# Check Locust
locust --version
# Should show: locust X.X.X (Python 3.x.x)

# Verify imports work
python3 -c "import yaml; print('‚úÖ PyYAML OK')"
python3 -c "import requests; print('‚úÖ Requests OK')"
python3 -c "from locust import HttpUser; print('‚úÖ Locust OK')"
```

### Virtual Environment Setup (Recommended)

Using a virtual environment isolates dependencies and is considered best practice:

```bash
# Create virtual environment
python3 -m venv venv

# Activate it
# On macOS/Linux:
source venv/bin/activate

# On Windows:
venv\Scripts\activate

# Install dependencies in virtual environment
pip install -r requirements.txt

# Verify installation
locust --version

# When done, deactivate
deactivate
```

### Dependencies

All packages are Python 3 compatible with specified minimum versions:

| Package | Version | Purpose |
|---------|---------|---------|
| **locust** | 2.0.0+ | Load testing framework |
| **pyyaml** | 5.4+ | YAML configuration parsing |
| **requests** | 2.25.0+ | HTTP client library |

All dependencies actively support Python 3 and have been tested with Python 3.7+.

## Configuration

### Environment Configuration (`config/env.yaml`)

Configure your API endpoint and authentication settings:

```yaml
host: https://api.example.com

auth:
  type: jwt
  token_url: /auth/login
  username: testuser
  password: password
```

**Parameters:**
- `host`: Base URL of the API to test
- `auth.type`: Authentication method (currently supports `jwt`)
- `auth.token_url`: Endpoint for obtaining JWT tokens
- `auth.username`: Username for authentication
- `auth.password`: Password for authentication

## Running Tests

### Quick Start
```bash
python runner/run.py
```

This command will:
1. Load configuration from `config/env.yaml`
2. Parse scenarios from `scenarios/users_api.yaml`
3. Execute load test with 50 concurrent users, 5 users spawning per second, for 1 minute
4. Generate HTML and CSV reports in `reports/` directory
5. Automatically validate results against SLA thresholds

### Custom Execution
Edit `runner/run.py` to customize:
- Number of users: `-u 50`
- Spawn rate: `-r 5` (users per second)
- Test duration: `-t 1m`
- Report location: `--html reports/report.html`

### SLA Validation

After tests complete, the framework automatically validates results against defined thresholds and reports:
- ‚úÖ **P95 Response Time**: 95th percentile latency against thresholds
- ‚úÖ **Error Rate**: Request failure rate against acceptable limits
- ‚úÖ **Detailed Violations**: Clear reporting of any SLA breaches

Run manual validation:
```bash
python runner/validate.py
```

## Test Scenarios

Test scenarios are defined in YAML format under `scenarios/users_api.yaml`:

```yaml
name: Users API Test

requests:
  - name: Get Users
    method: GET
    endpoint: /api/users
    weight: 3
    
  - name: Create User
    method: POST
    endpoint: /api/users
    payload:
      name: test
      role: user
    weight: 1
```

**Parameters:**
- `name`: Descriptive name for the request
- `method`: HTTP method (GET, POST, PUT, DELETE, etc.)
- `endpoint`: API endpoint path
- `weight`: Relative frequency of this request (higher = more frequent)
- `payload`: Request body (for POST, PUT requests)

### Weight Example
With weights of 3 and 1 above, "Get Users" will be called 3 times for every 1 "Create User" call.

## SLA Thresholds

Define performance requirements in `thresholds/sla.yaml`:

```yaml
Get Users:
  p95_ms: 800
  error_rate: 1

Create User:
  p95_ms: 1200
  error_rate: 2
```

**Parameters:**
- `p95_ms`: 95th percentile response time in milliseconds
- `error_rate`: Maximum acceptable error rate (percentage)

The framework validates test results against these thresholds and reports any violations.

## Reports & Metrics

### üìä Generated Reports

Test reports are generated automatically in the `reports/` directory after each test run:

#### 1. **performance_report.html** ‚≠ê **[MAIN REPORT]**
A beautiful, interactive HTML report featuring:
- **Executive Summary**: 6 color-coded metric cards
- **Detailed Metrics Table**: Per-endpoint breakdown
- **Response Time Distribution**: Min, Avg, P95, P99, Max
- **HTTP Method Badges**: Color-coded GET/POST/PUT/DELETE
- **Status Indicators**: Green/Yellow/Red for quick assessment
- **Modern Design**: Gradient background, responsive layout
- **Mobile Friendly**: Works on phones and tablets

```bash
# View the report
open reports/performance_report.html  # macOS
firefox reports/performance_report.html  # Linux
start reports/performance_report.html  # Windows
```

#### 2. **performance_report.json**
Machine-readable JSON format with:
- Timestamp of test execution
- Summary statistics (totals, rates, percentiles)
- Per-endpoint metrics
- Perfect for CI/CD integration and automation

#### 3. **report.html**
Locust's native HTML report with charts and statistics

#### 4. **results_stats.csv**
Summary statistics for all requests (raw data)

#### 5. **results_failures.csv**
Detailed information about failed requests

### Understanding Your Reports

#### üìå Executive Summary Cards
The HTML report displays 6 key metrics:

- **Total Requests**: Number of API calls made during test
- **Success Rate**: Percentage of successful requests
  - Green (‚â•99%): Excellent
  - Yellow (95-99%): Good
  - Red (<95%): Needs attention
- **Failed Requests**: Count of failed requests
- **Average Response Time**: Mean latency across all requests
- **P95 Response Time**: 95% of requests completed within this time
- **Max Response Time**: Longest request duration

#### üìä Detailed Metrics Table
Per-endpoint metrics including:
- **Requests**: Number of calls to this endpoint
- **Failures**: Failed requests count
- **Avg/Min/Max**: Response time statistics (milliseconds)
- **P95/P99**: 95th and 99th percentile latencies
- **Failure Rate**: Error percentage with color-coded status badge

#### üé® Visual Indicators
- üü¢ **Green Badges**: 0% failures - Healthy
- üü° **Yellow Badges**: <5% failures - Warning
- üî¥ **Red Badges**: ‚â•5% failures - Critical

#### Performance Summary Section
- **Min**: Fastest request
- **Avg**: Average latency
- **P95**: 95th percentile (95% of requests ‚â§ this time)
- **P99**: 99th percentile (99% of requests ‚â§ this time)
- **Max**: Slowest request

### Interpreting Response Time Metrics

Example interpretation:
```
Avg:  250ms  (average request takes 250ms)
P95:  600ms  (95% of requests ‚â§ 600ms, 5% took longer)
P99:  900ms  (99% of requests ‚â§ 900ms, 1% took longer)
Max:  3450ms (slowest single request was 3.45 seconds)
```

**Interpretation**: Good P95 (< 1000ms) means most users experience fast responses.

### Performance Benchmarks

| Metric | Excellent | Good | Acceptable | Poor |
|--------|-----------|------|-----------|------|
| Success Rate | >99.5% | >99% | >95% | <95% |
| P95 (ms) | <200 | <500 | <1000 | >1000 |
| P99 (ms) | <500 | <1000 | <2000 | >2000 |
| Error Rate | <0.5% | <1% | <5% | >5% |

### Generating Reports Manually

If you need to regenerate reports from existing test results:

```bash
# Generate fresh reports from last test results
python -m runner.report_generator
```

This is useful when:
- You want to re-analyze past test results
- You modified your analysis criteria
- You're comparing multiple test runs

### Viewing Reports in a Terminal

If you need to access reports without a GUI:

```bash
# Quick console summary
python -m runner.report_generator

# Extract metrics from JSON
python3 -c "
import json
with open('reports/performance_report.json') as f:
    data = json.load(f)
    print(f\"Success Rate: {data['summary']['success_rate']:.2f}%\")
    print(f\"Avg Response: {data['summary']['avg_response_time']:.0f}ms\")
    print(f\"P95 Response: {data['summary']['p95_response_time']:.0f}ms\")
"
```

## Authentication

### JWT Token Management

The framework supports JWT-based authentication via the `auth/jwt.py` module. Tokens are automatically:
1. **Obtained at startup**: Using configured credentials
2. **Included in headers**: Automatically attached to all requests
3. **Validated**: Checked for presence in response
4. **Refreshed**: As needed during test execution

### Configuration

Configure JWT settings in `config/env.yaml`:

```yaml
host: https://api.example.com

auth:
  type: jwt
  token_url: /auth/login
  username: your_username
  password: your_password
```

**Parameters:**
- `host`: Base URL of the API
- `auth.type`: Authentication method (currently `jwt`)
- `auth.token_url`: Endpoint for obtaining tokens
- `auth.username`: Username for authentication
- `auth.password`: Password for authentication

### Features

- ‚úÖ Automatic token retrieval at test start
- ‚úÖ Validation of token response
- ‚úÖ Error handling for auth failures
- ‚úÖ Timeout protection (10 second timeout)
- ‚úÖ Configuration validation
- ‚úÖ Clear error messages for debugging

### Troubleshooting Authentication

```bash
# If you see "Failed to obtain JWT token":
1. Verify auth.token_url is correct
2. Check username/password are valid
3. Ensure API is accessible from your machine
4. Check token endpoint returns 'access_token' field
```

## Troubleshooting

### Common Issues

**Issue: `ImportError: No module named 'locust'`**
```bash
# Solution: Install dependencies
pip install -r requirements.txt
```

**Issue: `FileNotFoundError: config/env.yaml`**
- Ensure you're running from the project root directory
- Check that all required configuration files exist

**Issue: `Connection refused` or `Failed to obtain JWT token`**
- Verify the API host in `config/env.yaml` is correct
- Check that credentials are valid
- Ensure the API is accessible and running

**Issue: SLA violations reported**
- Review the HTML report for performance details
- Check if the API is under load from other sources
- Consider increasing timeout thresholds in `thresholds/sla.yaml`
- Adjust test parameters (users, spawn rate) in `runner/run.py`

### Debug Mode

For more detailed output, modify `runner/run.py` to add verbose logging:
```python
result = subprocess.run([
    "locust",
    "-f", "locustfiles/dynamic_tasks.py",
    "--headless",
    "-u", "50",
    "-r", "5",
    "-t", "1m",
    "--html", "reports/report.html",
    "--csv", "reports/results",
    "-v"  # Add verbose flag
])
```

## Development

### Project Structure

```
api-perf-framework/
‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îî‚îÄ‚îÄ jwt.py                 # JWT authentication module
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ env.yaml               # Environment configuration
‚îú‚îÄ‚îÄ locustfiles/
‚îÇ   ‚îú‚îÄ‚îÄ base_api_user.py       # Base user class
‚îÇ   ‚îî‚îÄ‚îÄ dynamic_tasks.py       # Task definitions
‚îú‚îÄ‚îÄ runner/
‚îÇ   ‚îú‚îÄ‚îÄ run.py                 # Main test runner
‚îÇ   ‚îú‚îÄ‚îÄ validate.py            # SLA validation
‚îÇ   ‚îî‚îÄ‚îÄ report_generator.py    # Report generation
‚îú‚îÄ‚îÄ scenarios/
‚îÇ   ‚îî‚îÄ‚îÄ users_api.yaml         # Test scenarios
‚îú‚îÄ‚îÄ thresholds/
‚îÇ   ‚îî‚îÄ‚îÄ sla.yaml               # SLA thresholds
‚îú‚îÄ‚îÄ reports/                   # Generated test reports
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ‚îÄ README.md                  # This file
```

### Key Modules

#### `auth/jwt.py`
- Handles JWT token retrieval
- Validates configuration
- Provides error messages
- Includes timeout protection

#### `locustfiles/base_api_user.py`
- Base user class for all test users
- Handles configuration loading
- Manages JWT token setup
- Sets up HTTP headers

#### `locustfiles/dynamic_tasks.py`
- Implements weighted task distribution
- Executes API requests
- Handles different HTTP methods
- Includes payload support

#### `runner/run.py`
- Orchestrates test execution
- Creates reports directory
- Runs Locust with configured parameters
- Triggers report generation
- Validates SLA thresholds

#### `runner/report_generator.py`
- Parses Locust CSV output
- Calculates statistics
- Generates HTML reports
- Generates JSON reports
- Prints console summary

#### `runner/validate.py`
- Reads SLA thresholds
- Compares actual vs. expected metrics
- Reports violations
- Provides detailed feedback

### Python 3 Code Quality

The framework uses Python 3 best practices:
- **f-strings**: Clean string formatting
- **Type hints**: In docstrings for clarity
- **Context managers**: Proper resource handling (`with` statements)
- **Exception handling**: Specific exception types
- **Modern libraries**: subprocess, json, pathlib, datetime
- **Virtual environments**: Supported for isolation

### Extending the Framework

#### Adding New Endpoints

1. Edit `scenarios/users_api.yaml`:
```yaml
requests:
  - name: Get Profile
    method: GET
    endpoint: /api/user/profile
    weight: 2
```

2. Add SLA threshold in `thresholds/sla.yaml`:
```yaml
Get Profile:
  p95_ms: 600
  error_rate: 1
```

#### Adding Custom Authentication

1. Create new auth module in `auth/`:
```python
# auth/oauth2.py
def get_oauth_token(config):
    # Your OAuth2 implementation
    pass
```

2. Update `base_api_user.py` to use new auth method

#### Adding Custom Metrics

1. Modify `runner/report_generator.py` to add new metrics
2. Update HTML template to display new metrics
3. Regenerate reports with `python -m runner.report_generator`

### Testing Locally

```bash
# Run with minimal load (quick test)
# Edit runner/run.py: change -u 5 (instead of 50), -t 30s (instead of 60s)
python3 runner/run.py

# Test specific module
python3 -m pytest auth/jwt.py  # If tests exist

# Check for syntax errors
python3 -m py_compile auth/jwt.py
python3 -m py_compile locustfiles/*.py
python3 -m py_compile runner/*.py
```

## License

This project is open source and available under the MIT License.
