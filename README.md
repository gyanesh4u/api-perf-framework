# API Performance Testing Framework (Locust)

A comprehensive performance testing framework built on Locust for load testing APIs with support for JWT authentication, configurable scenarios, and SLA validation.

## Features

ğŸš€ **Production-Ready:**
- âœ… **Python 3.7+** - Fully compatible with Python 3.7, 3.8, 3.9, 3.10, 3.11, 3.12, 3.13+
- âœ… Weighted task distribution for realistic load patterns
- âœ… Automatic JWT token management and refresh
- âœ… Comprehensive error handling and validation
- âœ… SLA validation for both response time and error rates
- âœ… Beautiful interactive HTML reports with Chart.js graphs
- âœ… Timestamped report folders for historical tracking
- âœ… YAML-based configuration for easy management
- âœ… Modern Python 3 features (f-strings, context managers, pathlib)

## Table of Contents
- [Features](#features)
- [Quick Start](#quick-start)
- [Overview](#overview)
- [Project Structure](#project-structure)
- [Installation](#installation)
  - [Python 3 Setup](#python-3-setup)
- [Configuration](#configuration)
- [Running Tests](#running-tests)
- [Test Scenarios](#test-scenarios)
- [SLA Thresholds](#sla-thresholds)
- [Reports & Metrics](#reports--metrics)
- [Understanding Your Reports](#understanding-your-reports)
- [Authentication](#authentication)
- [Troubleshooting](#troubleshooting)
- [Development](#development)
- [License](#license)

## Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Update configuration
# Edit config/env.yaml with your API endpoint and credentials

# 3. Define test scenarios
# Edit scenarios/users_api.yaml with your API endpoints

# 4. Set SLA thresholds
# Edit thresholds/sla.yaml with your performance requirements

# 5. Run tests
python runner/run.py
```

## Overview

This framework provides a flexible, configuration-driven approach to API performance testing. It supports:
- Multiple API endpoints and HTTP methods
- JWT-based authentication
- Dynamic task allocation with weighted load distribution
- Customizable SLA thresholds
- HTML and CSV report generation
- YAML-based configuration for scenarios and thresholds

## Project Structure

```
api-perf-framework/
â”œâ”€â”€ auth/                     # Authentication modules
â”‚   â””â”€â”€ jwt.py              # JWT token handling
â”œâ”€â”€ config/                   # Configuration files
â”‚   â””â”€â”€ env.yaml            # Environment and API configuration
â”œâ”€â”€ locustfiles/             # Locust test definitions
â”‚   â”œâ”€â”€ base_api_user.py    # Base user class for API testing
â”‚   â””â”€â”€ dynamic_tasks.py    # Dynamic task generation
â”œâ”€â”€ runner/                   # Test execution and validation
â”‚   â”œâ”€â”€ run.py              # Main test runner
â”‚   â””â”€â”€ validate.py         # SLA validation
â”œâ”€â”€ scenarios/               # Test scenario definitions
â”‚   â””â”€â”€ users_api.yaml      # API endpoints and test cases
â”œâ”€â”€ thresholds/             # SLA configuration
â”‚   â””â”€â”€ sla.yaml            # Performance thresholds
â””â”€â”€ requirements.txt        # Python dependencies
```

## Installation

### Prerequisites
- **Python 3.7+** (3.8+ recommended, 3.10+ ideal)
- pip (Python package manager)

### Verify Python Installation

#### macOS/Linux:
```bash
python3 --version
# Output: Python 3.x.x (3.7 or higher)
```

#### Windows:
```bash
python --version
# Output: Python 3.x.x (3.7 or higher)
```

### Python 3 Setup

**Your framework is 100% Python 3 compatible** and uses modern Python 3 features throughout:
- f-strings for clean string formatting
- Context managers for proper resource handling
- Modern libraries (subprocess, json, pathlib, datetime)
- Comprehensive exception handling
- Type hints in docstrings

#### Recommended Python Versions:
- **Python 3.8**: Excellent stability, widely used
- **Python 3.9+**: Good performance improvements
- **Python 3.10+**: Recommended for best experience
- **Python 3.11+**: Latest, fastest version

### Installation Steps

#### Step 1: Install Dependencies
```bash
# Using pip3 (recommended)
pip3 install -r requirements.txt

# Or with Python module
python3 -m pip install -r requirements.txt
```

#### Step 2: Verify Installation
```bash
# Check Locust
locust --version
# Should show: locust X.X.X (Python 3.x.x)

# Verify imports work
python3 -c "import yaml; print('âœ… PyYAML OK')"
python3 -c "import requests; print('âœ… Requests OK')"
python3 -c "from locust import HttpUser; print('âœ… Locust OK')"
```

### Virtual Environment Setup (Recommended)

Using a virtual environment isolates dependencies and is considered best practice:

```bash
# Create virtual environment
python3 -m venv venv

# Activate it
# On macOS/Linux:
source venv/bin/activate

# On Windows:
venv\Scripts\activate

# Install dependencies in virtual environment
pip install -r requirements.txt

# Verify installation
locust --version

# When done, deactivate
deactivate
```

### Dependencies

All packages are Python 3 compatible with specified minimum versions:

| Package | Version | Purpose |
|---------|---------|---------|
| **locust** | 2.0.0+ | Load testing framework |
| **pyyaml** | 5.4+ | YAML configuration parsing |
| **requests** | 2.25.0+ | HTTP client library |

All dependencies actively support Python 3 and have been tested with Python 3.7+.

## Configuration

### Environment Configuration (`config/env.yaml`)

Configure your API endpoint and authentication settings:

```yaml
host: https://api.example.com

auth:
  type: jwt
  token_url: /auth/login
  username: testuser
  password: password
```

**Parameters:**
- `host`: Base URL of the API to test
- `auth.type`: Authentication method (currently supports `jwt`)
- `auth.token_url`: Endpoint for obtaining JWT tokens
- `auth.username`: Username for authentication
- `auth.password`: Password for authentication

## Running Tests

### Quick Start
```bash
python runner/run.py
```

This command will:
1. Load configuration from `config/env.yaml`
2. Parse scenarios from `scenarios/users_api.yaml`
3. Execute load test with 50 concurrent users, 5 users spawning per second, for 1 minute
4. Generate HTML and CSV reports in `reports/` directory
5. Automatically validate results against SLA thresholds

### Custom Execution
Edit `runner/run.py` to customize:
- Number of users: `-u 50`
- Spawn rate: `-r 5` (users per second)
- Test duration: `-t 1m`
- Report location: `--html reports/report.html`

### SLA Validation

After tests complete, the framework automatically validates results against defined thresholds and reports:
- âœ… **P95 Response Time**: 95th percentile latency against thresholds
- âœ… **Error Rate**: Request failure rate against acceptable limits
- âœ… **Detailed Violations**: Clear reporting of any SLA breaches

Run manual validation:
```bash
python runner/validate.py
```

### Report Organization

Each test run automatically creates a **timestamped folder** for reports:

```bash
# Run 1
python runner/run.py
# Creates: reports/2026-02-08_13-45-23/

# Run 2  
python runner/run.py
# Creates: reports/2026-02-08_13-47-58/

# All reports are organized by date and time
ls reports/
# 2026-02-08_13-45-23/
# 2026-02-08_13-47-58/
```

Access the latest report:
```bash
# Open latest performance report
open reports/*/performance_report.html  # macOS - opens most recent
firefox reports/2026-02-08_13-45-23/performance_report.html  # Linux
start reports/2026-02-08_13-45-23/performance_report.html  # Windows
```

Benefits of timestamped folders:
- ğŸ“… **Historical Tracking**: Keep all test results with date/time
- ğŸ” **Easy Comparison**: Compare performance across multiple runs
- ğŸ“Š **Trend Analysis**: See how performance changes over time
- ğŸ›¡ï¸ **Safety**: No reports are overwritten between runs

## Test Scenarios

Test scenarios are defined in YAML format under `scenarios/users_api.yaml`:

```yaml
name: Users API Test

requests:
  - name: Get Users
    method: GET
    endpoint: /api/users
    weight: 3
    
  - name: Create User
    method: POST
    endpoint: /api/users
    payload:
      name: test
      role: user
    weight: 1
```

**Parameters:**
- `name`: Descriptive name for the request
- `method`: HTTP method (GET, POST, PUT, DELETE, etc.)
- `endpoint`: API endpoint path
- `weight`: Relative frequency of this request (higher = more frequent)
- `payload`: Request body (for POST, PUT requests)

### Weight Example
With weights of 3 and 1 above, "Get Users" will be called 3 times for every 1 "Create User" call.

## SLA Thresholds

Define performance requirements in `thresholds/sla.yaml`:

```yaml
Get Users:
  p95_ms: 800
  error_rate: 1

Create User:
  p95_ms: 1200
  error_rate: 2
```

**Parameters:**
- `p95_ms`: 95th percentile response time in milliseconds
- `error_rate`: Maximum acceptable error rate (percentage)

The framework validates test results against these thresholds and reports any violations.

## Reports & Metrics

### ğŸ“Š Generated Reports

Test reports are generated automatically in timestamped folders under the `reports/` directory after each test run:

#### Report Folder Structure
```
reports/
â”œâ”€â”€ 2026-02-08_13-45-23/    # First test run (YYYY-MM-DD_HH-MM-SS)
â”‚   â”œâ”€â”€ performance_report.html
â”‚   â”œâ”€â”€ performance_report.json
â”‚   â”œâ”€â”€ report.html
â”‚   â””â”€â”€ results_stats.csv
â”œâ”€â”€ 2026-02-08_13-47-58/    # Second test run
â”‚   â”œâ”€â”€ performance_report.html
â”‚   â”œâ”€â”€ performance_report.json
â”‚   â”œâ”€â”€ report.html
â”‚   â””â”€â”€ results_stats.csv
â””â”€â”€ ...                      # More timestamped folders for each run
```

Each run creates a **unique timestamped folder** with format `YYYY-MM-DD_HH-MM-SS`, allowing you to keep a complete historical archive of all performance tests.

#### 1. **performance_report.html** â­ **[MAIN REPORT]**
A beautiful, interactive HTML report featuring:
- **Executive Summary**: 6 color-coded metric cards (Total Requests, Success Rate, Failures, Avg/P95/Max Response Time)
- **Interactive Charts** (using Chart.js):
  - ğŸ“Š **Requests Distribution**: Pie chart showing requests per endpoint
  - ğŸ“‰ **Response Time Trends**: Line chart comparing Avg, P95, P99, Max across endpoints
  - â±ï¸ **Response Time Breakdown**: Bar chart for each endpoint (Min/Avg/P95/P99/Max)
  - âœ… **Success Rate by Endpoint**: Bar chart with percentage breakdown
  - ğŸ“ˆ **Request Count by Method**: Bar chart showing GET/POST/PUT/DELETE distribution
- **Detailed Metrics Table**: Per-endpoint breakdown with all statistics
- **HTTP Method Badges**: Color-coded GET/POST/PUT/DELETE
- **Status Indicators**: Green/Yellow/Red for quick assessment
- **Modern Design**: Gradient background, responsive layout
- **Mobile Friendly**: Works on phones, tablets, and desktops
- **Generated Timestamp**: Date, time, and ISO timestamp of report generation

```bash
# View the report
open reports/performance_report.html  # macOS
firefox reports/performance_report.html  # Linux
start reports/performance_report.html  # Windows
```

#### 2. **performance_report.json**
Machine-readable JSON format with:
- Timestamp of test execution
- Summary statistics (totals, rates, percentiles)
- Per-endpoint metrics
- Perfect for CI/CD integration and automation

#### 3. **report.html**
Locust's native HTML report with charts and statistics

#### 4. **results_stats.csv**
Summary statistics for all requests (raw data)

#### 5. **results_failures.csv**
Detailed information about failed requests

### Understanding Your Reports

#### ğŸ“Œ Executive Summary Cards
The HTML report displays 6 key metrics:

- **Total Requests**: Number of API calls made during test
- **Success Rate**: Percentage of successful requests
  - Green (â‰¥99%): Excellent
  - Yellow (95-99%): Good
  - Red (<95%): Needs attention
- **Failed Requests**: Count of failed requests
- **Average Response Time**: Mean latency across all requests
- **P95 Response Time**: 95% of requests completed within this time
- **Max Response Time**: Longest request duration

#### ğŸ“Š Detailed Metrics Table
Per-endpoint metrics including:
- **Requests**: Number of calls to this endpoint
- **Failures**: Failed requests count
- **Avg/Min/Max**: Response time statistics (milliseconds)
- **P95/P99**: 95th and 99th percentile latencies
- **Failure Rate**: Error percentage with color-coded status badge

#### ğŸ¨ Visual Indicators
- ğŸŸ¢ **Green Badges**: 0% failures - Healthy
- ğŸŸ¡ **Yellow Badges**: <5% failures - Warning
- ğŸ”´ **Red Badges**: â‰¥5% failures - Critical

#### Performance Summary Section
- **Min**: Fastest request
- **Avg**: Average latency
- **P95**: 95th percentile (95% of requests â‰¤ this time)
- **P99**: 99th percentile (99% of requests â‰¤ this time)
- **Max**: Slowest request

### Interpreting Response Time Metrics

Example interpretation:
```
Avg:  250ms  (average request takes 250ms)
P95:  600ms  (95% of requests â‰¤ 600ms, 5% took longer)
P99:  900ms  (99% of requests â‰¤ 900ms, 1% took longer)
Max:  3450ms (slowest single request was 3.45 seconds)
```

**Interpretation**: Good P95 (< 1000ms) means most users experience fast responses.

### Performance Benchmarks

| Metric | Excellent | Good | Acceptable | Poor |
|--------|-----------|------|-----------|------|
| Success Rate | >99.5% | >99% | >95% | <95% |
| P95 (ms) | <200 | <500 | <1000 | >1000 |
| P99 (ms) | <500 | <1000 | <2000 | >2000 |
| Error Rate | <0.5% | <1% | <5% | >5% |

### Generating Reports Manually

If you need to regenerate reports from existing test results:

```bash
# Generate fresh reports from last test results
python -m runner.report_generator
```

This is useful when:
- You want to re-analyze past test results
- You modified your analysis criteria
- You're comparing multiple test runs

### Viewing Reports in a Terminal

If you need to access reports without a GUI:

```bash
# Quick console summary
python -m runner.report_generator

# Extract metrics from JSON
python3 -c "
import json
with open('reports/performance_report.json') as f:
    data = json.load(f)
    print(f\"Success Rate: {data['summary']['success_rate']:.2f}%\")
    print(f\"Avg Response: {data['summary']['avg_response_time']:.0f}ms\")
    print(f\"P95 Response: {data['summary']['p95_response_time']:.0f}ms\")
"
```

## Authentication

### JWT Token Management

The framework supports JWT-based authentication via the `auth/jwt.py` module. Tokens are automatically:
1. **Obtained at startup**: Using configured credentials
2. **Included in headers**: Automatically attached to all requests
3. **Validated**: Checked for presence in response
4. **Refreshed**: As needed during test execution

### Configuration

Configure JWT settings in `config/env.yaml`:

```yaml
host: https://api.example.com

auth:
  type: jwt
  token_url: /auth/login
  username: your_username
  password: your_password
```

**Parameters:**
- `host`: Base URL of the API
- `auth.type`: Authentication method (currently `jwt`)
- `auth.token_url`: Endpoint for obtaining tokens
- `auth.username`: Username for authentication
- `auth.password`: Password for authentication

### Features

- âœ… Automatic token retrieval at test start
- âœ… Validation of token response
- âœ… Error handling for auth failures
- âœ… Timeout protection (10 second timeout)
- âœ… Configuration validation
- âœ… Clear error messages for debugging

### Troubleshooting Authentication

```bash
# If you see "Failed to obtain JWT token":
1. Verify auth.token_url is correct
2. Check username/password are valid
3. Ensure API is accessible from your machine
4. Check token endpoint returns 'access_token' field
```

## Troubleshooting

### Python 3 Verification

First, verify you're running Python 3:

```bash
# Check Python version (macOS/Linux)
python3 --version
# Should show: Python 3.7 or higher

# Check Python version (Windows)
python --version
# Should show: Python 3.7 or higher

# Verify pip is Python 3
pip3 --version
# Should show: pip X.X from ... (python 3.X)

# If using Windows, check which Python interpreter is default
where python
# Should show path to Python 3.x
```

### Common Issues

**Issue: `ImportError: No module named 'locust'`**
```bash
# Solution: Install dependencies
pip install -r requirements.txt
```

**Issue: `FileNotFoundError: config/env.yaml`**
- Ensure you're running from the project root directory
- Check that all required configuration files exist

**Issue: `Connection refused` or `Failed to obtain JWT token`**
- Verify the API host in `config/env.yaml` is correct
- Check that credentials are valid
- Ensure the API is accessible and running

**Issue: SLA violations reported**
- Review the HTML report for performance details
- Check if the API is under load from other sources
- Consider increasing timeout thresholds in `thresholds/sla.yaml`
- Adjust test parameters (users, spawn rate) in `runner/run.py`

### Debug Mode

For more detailed output, modify `runner/run.py` to add verbose logging:
```python
result = subprocess.run([
    "locust",
    "-f", "locustfiles/dynamic_tasks.py",
    "--headless",
    "-u", "50",
    "-r", "5",
    "-t", "1m",
    "--html", "reports/report.html",
    "--csv", "reports/results",
    "-v"  # Add verbose flag
])
```

## Development

### Project Structure

```
api-perf-framework/
â”œâ”€â”€ auth/
â”‚   â””â”€â”€ jwt.py                 # JWT authentication module
â”œâ”€â”€ config/
â”‚   â””â”€â”€ env.yaml               # Environment configuration
â”œâ”€â”€ locustfiles/
â”‚   â”œâ”€â”€ base_api_user.py       # Base user class
â”‚   â””â”€â”€ dynamic_tasks.py       # Task definitions
â”œâ”€â”€ runner/
â”‚   â”œâ”€â”€ run.py                 # Main test runner
â”‚   â”œâ”€â”€ validate.py            # SLA validation
â”‚   â””â”€â”€ report_generator.py    # Report generation
â”œâ”€â”€ scenarios/
â”‚   â””â”€â”€ users_api.yaml         # Test scenarios
â”œâ”€â”€ thresholds/
â”‚   â””â”€â”€ sla.yaml               # SLA thresholds
â”œâ”€â”€ reports/                   # Generated test reports
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

### Key Modules

#### `auth/jwt.py`
- Handles JWT token retrieval
- Validates configuration
- Provides error messages
- Includes timeout protection

#### `locustfiles/base_api_user.py`
- Base user class for all test users
- Handles configuration loading
- Manages JWT token setup
- Sets up HTTP headers

#### `locustfiles/dynamic_tasks.py`
- Implements weighted task distribution
- Executes API requests
- Handles different HTTP methods
- Includes payload support

#### `runner/run.py`
- Orchestrates test execution
- Creates reports directory
- Runs Locust with configured parameters
- Triggers report generation
- Validates SLA thresholds

#### `runner/report_generator.py`
- Parses Locust CSV output
- Calculates statistics
- Generates HTML reports
- Generates JSON reports
- Prints console summary

#### `runner/validate.py`
- Reads SLA thresholds
- Compares actual vs. expected metrics
- Reports violations
- Provides detailed feedback

### Python 3 Code Quality

The framework uses **modern Python 3 best practices** throughout:

#### Language Features Used
- **f-strings**: Clean string formatting (Python 3.6+)
  ```python
  # Instead of: "Date: {}".format(date)
  print(f"Generated on: {now.strftime('%Y-%m-%d %H:%M:%S')}")
  ```
- **Type hints**: In docstrings for clarity
  ```python
  def parse_csv_reports(reports_dir="reports"):
      """
      Parse Locust CSV reports and extract metrics.
      """
  ```
- **Context managers**: Proper resource handling
  ```python
  with open(stats_file) as f:
      reader = csv.DictReader(f)
  ```
- **pathlib**: Modern path handling instead of os.path
  ```python
  from pathlib import Path
  ```
- **Walrus operator** (Python 3.8+): Assignment within expressions
- **Dataclasses** (Python 3.7+): Clean data structures

#### Standard Library Usage
- **subprocess**: Modern subprocess execution with proper error handling
- **json**: Built-in JSON parsing without external dependencies
- **csv**: DictReader for clean CSV parsing
- **datetime**: Native datetime handling with timezone support
- **os**: Directory and file operations
- **sys**: System-level operations

#### Tested Python Versions
| Version | Status | Notes |
|---------|--------|-------|
| 3.7 | âœ… Full Support | Minimum supported version |
| 3.8 | âœ… Full Support | Excellent stability |
| 3.9 | âœ… Full Support | Good performance |
| 3.10 | âœ… Full Support | Performance improvements |
| 3.11 | âœ… Full Support | Faster by default |
| 3.12 | âœ… Full Support | Latest stable |
| 3.13 | âœ… Full Support | Current preview (when stable) |

### Extending the Framework

#### Adding New Endpoints

1. Edit `scenarios/users_api.yaml`:
```yaml
requests:
  - name: Get Profile
    method: GET
    endpoint: /api/user/profile
    weight: 2
```

2. Add SLA threshold in `thresholds/sla.yaml`:
```yaml
Get Profile:
  p95_ms: 600
  error_rate: 1
```

#### Adding Custom Authentication

1. Create new auth module in `auth/`:
```python
# auth/oauth2.py
def get_oauth_token(config):
    # Your OAuth2 implementation
    pass
```

2. Update `base_api_user.py` to use new auth method

#### Adding Custom Metrics

1. Modify `runner/report_generator.py` to add new metrics
2. Update HTML template to display new metrics
3. Regenerate reports with `python -m runner.report_generator`

### Testing Locally

```bash
# Run with minimal load (quick test)
# Edit runner/run.py: change -u 5 (instead of 50), -t 30s (instead of 60s)
python3 runner/run.py

# Test specific module
python3 -m pytest auth/jwt.py  # If tests exist

# Check for syntax errors
python3 -m py_compile auth/jwt.py
python3 -m py_compile locustfiles/*.py
python3 -m py_compile runner/*.py
```

## License

This project is open source and available under the MIT License.
